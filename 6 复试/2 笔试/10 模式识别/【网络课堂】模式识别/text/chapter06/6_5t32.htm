<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><!-- InstanceBegin template="/Templates/frame_content.dwt" codeOutsideHTMLIsLocked="false" -->
<head>
<!-- InstanceBeginEditable name="doctitle" -->
<title>Untitled Document</title>
<!-- InstanceEndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<!-- InstanceBeginEditable name="head" -->
<!-- InstanceEndEditable --> 
<link href="../../css/fc.css" rel="stylesheet" type="text/css">
<style type="text/css">
<!--
.bg {
	background-image: url(../../images/images/main__11.gif);
	background-position: right bottom;
	background-repeat: no-repeat;
	background-attachment: fixed;
}
-->
</style>
</head>

<body >
<table width="100%" height="100%" border="0" cellpadding="4" cellspacing="0">
  <tr>
    <td valign="top"><!-- InstanceBeginEditable name="text" -->
      <table width="100%" border="0" cellspacing="0" cellpadding="0">
        <tr>
          <td class="FCcontent"><strong>6.5.3.2 自适应共振理论</strong><br>
            　　这是另一个非监督竞争学习方法的例子，只考虑有兴趣的同学参考。<br>
            　　使用前面讨论的非监督竞争学习方法，会出现分类不稳定的问题。每当输入模式输入，则网络的权值有所修改。这种永无止境的修改是不希望的。当然通过逐渐将修改系统η降低并趋于零的方法，可以使网络稳定下来。但这种强制性又使网络失去变通性，也就是说网络失去对新的输入数据作出应变的能力。因此稳定性与变通性就成为一对矛盾，显然这是网络付诸实际应用需要解决的问题。<br>
            　　联想到人对事物的分类与记忆，可以构造一个网络，既使网络的工作状态具有稳定性，同时又有变通性。Carpetcr与Grossberg在1987年前后提出的自适应共振理论(Adaptive 
            Resonance Theory)及相应的网络模型ATR1与ATR2就是为了解决这一矛盾而提出的。他们提出的模型的基本点是：<br>
            　　(1) 网络中设置了一个识别层(Recognition layer)。它由一组结点组成。每个结点可以处于两种状态：已存模式与备用两种状态。在网络未经训练的情况下(称为初始状态)，所有结点都处于备用状态。而在训练过程中，其中的某些结点会从备用状态转至已存模式状态，因此它会对输入的模式作出有选择的响应，从而实现对输入模式的分类。如果当前输入的模式与所存储的模式皆不符合，则备用的结点又可对其作出响应。<br>
            　　(2) 对已存储有模式的结点可以修改其所存的模式。但是修改是有条件的，只有当当前输入的模式与该结点已存模式足够相似，该输入模式才被确认为由该结点所代表的模式类别，并依据它对已存模式作适当的修改，以便能更好地反映这一类模式。<br>
            从以上这两条可以看出，第二条用来保持已存模式的相对稳定性，而第一条则用备用结点的设置满足网络应具有的扩展功能，对新事物作出反映的能力。Carpenter与Grossberg为了实现他们提出的这种设想，先后提出了适用于二进制工作的ART1模式，以及能在连续值下工作的ART2模式。本节只讨论ART1模型，以说明其工作原理。实际上Carpenter等人在提出它们的理论时，并不十分具体。因此其它人提出了许多不同的实现方法。本节主要参照Lippman(1987)提出的实现方法。<br>
            　　一个简化了的ART1模型可表示成图6-17所示。</td>
        </tr>
        <tr>
          <td align="center"><img src="../../images/image_content/6/6_5059.gif" width="432" height="205"><br>
            <span class="FCcontent">图6-17 简化的ART网络</span></td>
        </tr>
        <tr>
          <td class="FCcontent">　　ART模型中的主要组成为一个比较层(Comparison Layer)、一个识别层(Recognition Layer)、以及三个起控制作用的部件：控制门1(Cate 
            1)、控制门2(Cate 2)以及重置部件(Reset)。它们之间的联系如图6-17所示。其中比较层接受从外部输入的输入模式X，从G1来的控制信号G1以及从识别层来的信号R。比较层的输出为C，它输出到识别层去。识别层除了接收C向量外，还接受从控制门2来的G2信号及从重置部件来的信号。比较层及识别层的简化框图如图6-18及图6-19所示。ART模型的工作过程又可分成初始化、识别、比较、搜索与训练5个组成部分，其中识别、比较这两个核心是通过网络中三组连接体现出来的，这三组连接分别表示成T<sub>ij</sub>、b<sub>ij</sub>以及识别层内的竞争连接机制。T<sub>ji</sub>是识别层中第j位结点rj到比较层中第i结点之间的连接权值，在ART1中只取0或1两种状态，b<sub>ij</sub>则是设置在识别层内从其输入端C<sub>i</sub>到第j个结C<sub>i</sub>之间的连接。在识别过程中识别层接受C向量，此时的C向量实质就是从外部输入的输入模式向量X。由于连接权值b<sub>j</sub>的作用，识别层中的不同结点对同一C向量会作出各自不同的响应。而在结点之间的竞争连接机制又通过胜者获取一切的原则使其中只有唯一一个结点rj激活，并使该结点呈现出“1”状态，而其余的结点则全为“0”。这就是识别的主要过程。然而识别的结果是要通过比较加以检验的，这可通过连接权值Ti来实现。连接权值向量T<sub>j</sub>是比较层中的组成部分，它的值也就是该结点的存储的模式的标准形式，因而T<sub>j</sub>组成了一个N位的二进制向量。该二进制向量与输入模式在比较层的结点中进行比较，这种比较通过逐位的“与”运算加以实现，反映了两者在每一位的异同，如果输入模式与相应的T<sub>j</sub>之间差异甚小，则表明当前的输入模式与该存储模式很相似，从而识别分类有效，此时相应的标准模式应依据当前输入模式作适当的修正，这就是训练。如输入模式与相应的T<sub>j</sub>之间差异越过一定阈值，则该输入模式就不能归入这一类，需要搜索其它类别。如无合适类别，则要启用备用结点，另建新类。以上只是ART模型式工作过程的大致情况，它的整个工作进程还需通过各个部件的功能加以进一步说明</td>
        </tr>
        <tr>
          <td align="center" class="FCcontent"><img src="../../images/image_content/6/6_5060.gif" width="450" height="375"><br>
            图6-18 比较层简化框图<br> <img src="../../images/image_content/6/6_5061.gif" width="451" height="364"> 
            <br>
            图6-19 识别层简化框图</td>
        </tr>
        <tr>
          <td class="FCcontent">　　为了说明ART模型的工作全过程，首先需要对控制门1、控制门2以及重置部件的功能加以说明。控制门1的作用是对比较层内的网络结点进行控制。控制门1的输出接到每个结点的控制端，控制门1的输出因而也是一个N位的二进制向量，但它只能取全“0”或全“1”两种状态。控制门1的输出状态取决于识别层输出的R以及外部输入的X。只有R中每位都为“0”，同时X中至少有一位不为“0”的条件下，G1才为“1”，而其它情况下G1就变为全“0”。对控制门1的这种设计是为比较层结点进行不同的操作设计的，它的控制作用是通过所谓“三分之二”原则实现的。对比较层中的每个结点来说，除了从控制门1来的信号以及从外部输入的模型X外，还有一组P信号，它取决于R中哪一位为“1”以及与其有关的T<sub>j</sub>信号。这三种信号通过所谓三分之二原则决定其输出的状态。例如当G1为全“1”状态时，只要输入模式这一位X<sub>i</sub>为“1”，则相应的输出c<sub>i</sub>＝X<sub>i</sub>，因此X向量也就忠实地被传递到C向量上。如果G1处于全零状态，则ci的状态就取决于其它两种信号的状态，此时依据“三分之二”原则，C<sub>i</sub>为“1”就必需要相应的X<sub>i</sub>及Pi都为“1”。换句话说在G1为零的条件下，比较层的结果对其它两种信号进行按位的“与”运算，通过这种方式对输入模式与所存模式进行了比较。<br>
            　　控制门2的作用相对简单。它的功能是使识别层中的所有结点都处于使能状态，因而它们能对C向量通过相应的连接权向量b<sub>j</sub>作出响应。控制门2的输出只有当X向量至少有一位不为“0”时为“1”，从而起到上述作用。重置部件的主要作用是对C向量与X向量进行比较，具体说来是分别统计C向量及X向量中处于“1”状态分量的个数，计算这两者的比值，并与一阈值ρ进行比较，按该比值与ρ之间大小关系对识别层进行控制。<br>
            　　对上述控制模块的功能加以说明之后，ART模型的工作过程可以归纳为：<br>
            　　1) 初始化：在初始化阶段将所有T<sub>ji</sub>设置为“1”，而b<sub>ij</sub>则按以下方式设置<br>
            　　<img src="../../images/image_content/6/6_5062.gif" width="129" height="34" align="absmiddle"><br>
            　　其中N是输入模式位数，L是一个大于1的常数，可用L＝2。初始化时将识别层中所有结点初始至“使能”状态，即处于可与输入信号C进行运算的状态。<br>
            　　2) 识别：在识别过程中对所有处于“使能”状态的识别层结点进行以下点积运算：<br>
            　　<img src="../../images/image_content/6/6_5063.gif" width="118" height="34" align="absmiddle">　　(6-72)<br>
            　　其中b<sub>j</sub>是与j结点连续的权向量，C是识别层的输入向量，在此时也就是输入模式向量。<br>
            　　3) 竞争：识别层中结点相互竞争，唯一的胜者<img src="../../images/image_content/6/6_5064.gif" width="17" height="25" align="absmiddle">具有<img src="../../images/image_content/6/6_5065.gif" width="39" height="32" align="absmiddle">，而其余结点<img src="../../images/image_content/6/6_5066.gif" width="89" height="33" align="absmiddle">。<br>
            　　4) 比较：由r<sub>j</sub>＝1的结点所对应的T<sub>j</sub>向量成为在比较层结点的P向量，它在比较层中与输入模式进行比较，经过逐位“与”操作，C向量体现了P与X的按位“与”结果，C将与X在重置部个中进行比较，如有<br>
            　　<img src="../../images/image_content/6/6_5067.gif" width="84" height="37" align="absmiddle">　　(6-73)<br>
            　　其中‖c‖及‖X‖分别是C及X中“1”的个数的总和，ρ是一个指定阈值。如上式成立，则转至训练，否则转至搜索。<br>
            　　5) 搜索：如(6-73)不成立，则重置部件将识别层中的当前胜者结点封闭，并转至识别。<br>
            　　6) 训练：对识别层中获胜的结点的相应权向量进行修正，其中T<sub>ij</sub>=c<sub>i</sub>，而<br>
            　　<img src="../../images/image_content/6/6_5070.gif" width="172" height="39" align="absmiddle">　　(6-74)<br>
            　　其中L是一个大于1的常数，通常L＝2。需注意的是以上设置的b<sub>j</sub>能保证备用结点在竞争中不会取胜。<br>
            　　ART模型的性能有一些主要的特点：<br>
            　　1) 在训练已达稳定后，应用训练中使用过的模式或具备已存储模式基本特征的模式将立即被正确识别，而不需要一个搜索过程；<br>
            　　2) 搜索过程是稳定的，这是指一旦识别层中的胜者已被选出，不管C向量在比较层中发生变化，胜者不会发生改变。<br>
            　　3) 训练也是稳定的，训练也不会引起胜者发生变化；<br>
            　　4) 训练过程是能结束的，重复输入训练序列不会使ART中的权值无休止地循环变化。<br>
            　　ART模型的提出对解决稳定性与变通性这一对矛盾是有意义的，它也是为了模仿脑子的功能而提出的。但是脑子中的神经元是以分布的方式起作用的，而ART中一旦识别层中某个结点损坏了，则相应的记忆就会丢失，这一点与生物脑的作用是不一样的。<br>
            　　还需提到的是ART模型中对T<sub>j</sub>及B<sub>j</sub>这两个向量的初始化与训练方法是经过精心设计的。Carpenter和Grossberg曾证明过，如T<sub>j</sub>取太小的值将导致比较层中无法实现模式匹配与训练，而B<sub>j</sub>的值不能太大，否则会将识别层的所有结点存储同一个模式。此外Carpenter等对训练过程定义了两种方式，一种称为慢训练方式，即使用其它非监督学习方法常用的修正权值的方式。此时T<sub>j</sub>将不再是二进制方式，而且它也不再简单遵从某个输入模式，而是所有有关输入模式的统计效果。本节中讨论的训练方式称为快训练。此时T<sub>j</sub>只能取二进制向量的形式。<br>
            　　ART模型的性能与相似性阈值ρ的选取也有很大关系，问题在于没有合适的理论来指导ρ值的选取方法。Carpenter等曾建议用所谓分类结果正确与不正确，引入一惩罚函数来影响ρ的选择，但不可能超出了非监督学习方法的范围。</td>
        </tr>
      </table>
      <!-- InstanceEndEditable --></td>
  </tr>
</table>
</body>
<!-- InstanceEnd --></html>
