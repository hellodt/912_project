<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><!-- InstanceBegin template="/Templates/frame_content.dwt" codeOutsideHTMLIsLocked="false" -->
<head>
<!-- InstanceBeginEditable name="doctitle" -->
<title>Untitled Document</title>
<!-- InstanceEndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<!-- InstanceBeginEditable name="head" -->
<!-- InstanceEndEditable --> 
<link href="../../css/fc.css" rel="stylesheet" type="text/css">
<style type="text/css">
<!--
.bg {
	background-image: url(../../images/images/main__11.gif);
	background-position: right bottom;
	background-repeat: no-repeat;
	background-attachment: fixed;
}
-->
</style>
</head>

<body >
<table width="100%" height="100%" border="0" cellpadding="4" cellspacing="0">
  <tr>
    <td valign="top"><!-- InstanceBeginEditable name="text" -->
      <p class="FCcontent">　　传统的模式识别技术中的学习方法有监督与无监督两种，同样人工神经网络的学习方法也可分成这两种。典型的有监督学习方法可以以误差回传算法为例，如使用前向网络实现分类的情况。在这种应用的训练过程中，输入信号与其相应的输出之间的映射关系是指定的，而训练的目的则是调整网络内的结构及各种参数，以实现这种映射关系。<br>
        　　非监督学习方法与有监督学习方法的最根本区别在于没有“教师”来指导网络的学习，网络的学习目的并不着眼于实现某种明确指定的映射关系，而是要分析与确定数据内部带结构性与规律性的关系，从而能有效地利用这种带规律性的知识。<br>
        在原始的数据中摸索其规律性又可分成几种不同的情况：<br>
        　　1) 聚类：聚类的目的在于确定原始数据在其度量空间中是否存在集群分布的现象。如果集群现象存在，则将相应的数据集分成相应数量的数据子集。如果将原始数据的度量空间，或特征空间用d维的实数空间表示，则其聚类结果可以用相应的代号集表示。这是一种从Rd空间到一个个数有限的整数集的映射。<br>
        原始数据是否存在聚类现象，主要反映具有相同性质的原始数据受随机因素的影响，呈现出度量值的离散性。因此确定其聚类性质，从而可以发现数据的本质与内在规律。<br>
        　　2) 确定数据原型：对数据进行聚类分析并确定了相应的类别之后，每一种类别还可以选择出一种代表型数据来概括这一类数据的典型值。这种典型性表示方法一般能更准确地反映这一类数据的本质性内容，或其内涵。在这种情况下我们可以把映射关系表示成从Rd空间映射到同一个Rd空间，所不同的在于原始数据在该空间中呈分布状态，因而可以说是在该空间的连续分布，而其原型只存在于该空间中的若干离散点中。<br>
        　　3) 编码：如果进一步将上述原型用某种代码表示，并希望这种代码能尽可能多地保留原数据所包含的信息，这又是编码的特定需要。在这种应用中强调的是信息的压缩，以利于传输、管理与存储。此时的映射关系则很可能是在原d缩空间内部，也可能从原Rd空间转换到另一个代码所表示的空间。实现这种映射关系是为了让数据以一种更紧凑的方式表达，但又要尽可能避免信息的损失。<br>
        　　4) 特征映射：特征映射与前几种映射不同之处在于，它强调输出结点要形成一种几何分布，例如结点组成一张二维的矩形网，并利用输出结点之间的相邻近程度来反映原输入模式之间的相似程度。例如结点a与b分别是对输入信号A与B的响应，如果A与B是很相似的，则要求a与b应在输出结点结构中也呈现出相邻关系。这种映射关系并不着眼于输入与输出之间是一种怎样的映射，但却强调输出结点的几何相邻性反映输入信号之间的相似性。从而可用输出结点的分布特点反映复杂信号的内部联系。<br>
        　　5) 主分量分析：在前面的章节中曾提到主分量分析(Principal Component Analysis)，也叫K-L变换。利用人工神经网络也可实现主分量分析的功能，其学习方法也属非监督学习类型。主分量分析实质上是对原数据使用的度量空间的基进行变换，从而在数据用变换后的基函数描述后，能做到使信号的降维描述损失最小。<br>
        　　综上所述可以看出，用非监督学习方法建立的神经网络的主要功能是从原始数据中提炼出有用的信息。它可用来消除原数据中存在的大量的冗余信息，揭示出数据集的内涵，找出数据集中体现的规律性与结构信息，对数据集的统计参数进行估计等。另一方面也正是数据集中存在的大量冗余信息，才会体现出数据内存在的规律性，从而为非监督学习抽取这些规律提供了依据。<br>
        一般说来，使用非监督学习方法的人工神经网络的结构都比较简单。多数网络为一个单层结构；输出结点的数量也往往要比输入信号的维数要少。但也有例外的情况，例如特征映射所使用的网络往往有数量很大的输出结点集。另外著名的自适应共振理论模型(Adaptive 
        Resonance Theory，简称ART)的结构也相当复杂。<br>
        　　尽管非监督学习的结构简单，但所使用的学习规则却丰富多采。从原理上又可分为两大类，一类学习方法中包含了竞争机制，采用优胜劣汰的原则，用英文来说是Winner-take-all，即胜者占有一切的机制。而另一类则没有使用这种机制。从原理上讲它是使用了Hebb类型的规则，因此称为非监督Hebb学习(Unsupervised 
        Hebbian Learning)，而使用了竞争机制的方法则称为非监督竞争学习(Unsupervised Competitive Learning)。</p>
      <!-- InstanceEndEditable --></td>
  </tr>
</table>
</body>
<!-- InstanceEnd --></html>
