<html>
<head>
<title>Untitled Document</title>
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<link rel="stylesheet" href="../../../../css/text.css" type="text/css">
</head>

<body bgcolor="#FFFFFF" text="#000000" leftmargin="5" topmargin="5" marginwidth="5" marginheight="5">
<table width="100%" border="0" cellspacing="0" cellpadding="0">
  <tr> 
    <td class="text"><font color="#003399"><b>　　3.3.6.1 概述</b></font><a name="01"></a><br>
      　　国际标准化组织/国际电工委员会所属WG11工作组，制定推荐了MPEG标准。已公布和正在讨论的标准有 MPEG I、MPEG II、MPEG 
      IV、MPEG VII。本节介绍的内容是MPEG I标准的一部分，对应于ISO/IEC 11172-3(MPEG－音频)。这部分规定了高质量音频编码方法，存储表示和解码方法。编码器的输入和解码器的输出与现存的PCM标准兼容。ISO/IEC 
      11172视频、音频的总数据率为1.5Mb/s。音频使用的采样率为32kHz,44.1kHz和48kHz。编码输出的数据率有许多种，由相关的参数决定。 
      <br>
      <font color="#003399">　　(1)编码器</font><br>
        　　编码器处理数字音频信号，并生成存储所需的数据流。但编码器的算法并没有标准化，可以使用多种算法，如对音频掩蔽阈值估计的编码、量化和缩放。只要编码器输出的数据能使符合本标准的解码器解出适用的音频流。图3.14表明了音频编码器的基本结构。编码过程如下：输入的音频抽样被读入编码器。映射器建立经滤波的输入音频数据流的子带抽样表示。如在层Ⅰ、层Ⅱ，则是子带抽样，在层Ⅲ是经变换的子带抽样。心理声学模型建立一组控制量化和编码的数据。这些数据随实际编码器而变。一种可能的办法是利用音频掩蔽阈值来控制量化器。量化和编码部分是从已映射的输入抽样中生成一组编码符号。这部分也与编码系统有关。帧封装将来自其它模块的输出数据汇集成实际数据，如果需要的话，再加上其它信息，如校正信息。最后输出已编码的数据流。<br>
      　有四种不同的编码模式：单声道模式、双声道模式、立体声模式和联合立体声模式。<br>
      <font color="#003399">　图3.17 音频编码器基本结构框图</font> <br>
      　　<img src="../../../../images/chatp/chap03/095.gif" width="319" height="141"> 
      <br>
      <font color="#003399">　　(2)编码层次：</font><br>
      　　根据应用需求，可以使用不同层次的编码系统，编码器的复杂性和性能也随之升高。<br>
      　　・层Ⅰ包括将数字音频变成32个子带的基本映射。将数据格式化成块的固定分段。决定自适应位分配的心理声学模型。利用块压扩和格式化的量化器。理论上，层Ⅰ编码/解码的最少延时约为19ms。<br>
      　　・层Ⅱ提供了位分配，缩放因子和抽样的附加编码。使用了不同的帧格式。这层理论上的最小编码/解码延时约为35ms。<br>
      　　・层Ⅲ采用混合带通滤波器来提高频率分辨率。它增加了差值量化(非均匀)、自适应分段和量化值的熵编码。这层理论上的最小编码/解码延时为59ms。<br>
      　　联合立体声编码作为一个附加特性，能够加入到任何一层中。
      <br>
　　(3) 存储：已编码的视频数据、音频数据、同步数据、系统数据和辅助数据均可一并存入同一存储介质中。如果限定编辑点与可寻地点一致，音频编辑是很容易的。<br>
        　　对存储器的存取可能包括在通信系统中的远程存取。假定存取被一个功能单元控制，而不是被音频解码器本身控制。这个控制单元接收用户命令，读取并解释数据的基本结构信息，从介质中读取已存储的信息，分解非音频信息，按所需的速率将存储的音频数据流传送给音频解码器。<br>
        　　<br>
      <font color="#003399">　　(4)解码：</font><br>
      　　解码器按编码器定义的语法接收压缩的音频数据流，按解码部分的方法解出数据元素，按滤波器的规定，用这些信息产生数字音频输出。<br>
      　　图3.18　表明了音频解码器的基本结构。其解码过程如下：数据流输入到解码器。首先进行数据流拆封，恢复出各种信息。如果在编码器中使用了误差校验，解码器也将进行误差校验。重构单元将重构一组映射抽样的量化方案。逆映射单元把这些抽样变换回均匀PCM。<br>
      <font color="#003399">　图3.18 音频解码器结构框图</font><br>
      　　<img src="../../../../images/chatp/chap03/096.gif" width="335" height="106"><br>
      <font color="#003399"><b>　　3.3.6.2 音频表示</b></font> <a name="02"></a><br>
      　　在MPEG标准中,规范了编码数据表示。编码数据包括缩放因子、音频数据编码和辅助信息等。在MPEG标准中，声音数据是以帧为单位传送的，叫音频帧(frame)。不管采用哪一层规定的编码方法，音频帧的定义都是一样的。而各层之间的差别主要在音频数据的格式上。<br>
      　　音频帧的简化结构如图3.19。所示。音频表示是后面将要介绍的编码、解码的基础。针对三层不同的编码方法，分别介绍其数据表示。<br>
      　<font color="#003399">图3.19 音频帧的简化结构图</font> <br>
      　<img src="../../../../images/chatp/chap03/097.gif"><br>
      　　(1)音频帧<br>
      　　音频帧由头(header)、错误校验(error_check)、音频数据(audio_data)和辅助数据(ancillary_data)组成:<br>
      一个音频帧，在层Ⅰ中含有384个样点的信息，在第Ⅱ层和第Ⅲ层中均含有1152个样点的信息。值得注意的是在层Ⅲ中，属于一帧的音频数据并不总是包含在两个连续的同步字之间。
      <br>
      　　(2)音频头<br>
      　　头包含同步和状态信息。对所有的编码层来讲，音频帧开始的32位（4字节）都是头信息。它包括：<br>
      　　同步字(syncword)、<br>
      　　算法标识符(ID)、<br>
      　　层号(layer)、<br>
      　　保护位(protection_bit)、<br>
      　　位率索引(bitrate_index)、<br>
      　　采样频率(Sampling_frequency)、<br>
      　　填充位(padding_bit)、<br>
      　　私有位(private_bit)、<br>
      　　模式(mode)、<br>
      　　模式扩展(mode_extension)、<br>
      　　版权（Copyright)、<br>
      　　原始流/复制品(original/copy)、<br>
      　　增强(emphasis)。 
      <br>
　　(3)　错误校验<br>
        　　错误校验是一个16位的奇偶检验字，采用循环冗余码。<br>
        　　(4) 层Ⅰ的音频数据<br>
      　　这一层的音频数据包括：<br>
      　　位分配(allocation[ch][sb])、<br>
      　　缩放因子(scalefactor[ch][sb])、<br>
      　　音频编码数据(sample[ch][sb][s])。<br>
        　　(5)　层Ⅱ的音频数据<br>
      　　这一层的音频数据包括：<br>
      　　位分配(allocation[ch][sb])、<br>
      　　缩放因子选择信息(scfsi[ch][sb])、<br>
      　　缩放因子(scalefactor[ch][sb][p])、<br>
      　　成组(grouping[ch][sb])、<br>
      　　样点码字(sample code[ch][sb][gr])、<br>
      　　抽样编码（sample[ch][sb][s])。<br>
        　　(6)　层Ⅲ的音频数据<br>
        　　这一层的音频数据包括：<br>
      　　主数据开始(main_data_begin)<br>
      　　私有位(private_bits)<br>
      　　缩放因子选择(scfsi[ch][scfsi_band])<br>
      　　缩放因子选择组(scfsi_band) 　 part2_3_length[gr][ch]<br>
      　　大值(big_valuse[gr][ch])<br>
      　　全局增益(global_gain[gr](ch])<br>
      　　缩放压缩因子(scalefac_compress[gr][ch])<br>
      　　窗切换标志(window_switching_flag[gr][ch])<br>
      　　块类型(block_type[gr][ch])<br>
      　　混合块标志(mixed_block_flag[gr][ch])<br>
      　　表选择(table_select[gr][ch][region])<br>
      　　子块增益(subblock_gain[gr][ch][window])<br>
      　　0区域计数(region0_count[gr][ch])<br>
      　　1区域计数(region1_count[gr][ch])<br>
      　　预标志(preflag[gr][ch])<br>
      　　缩放因子_缩放(scalefac_scale[gr][ch])<br>
      　　计数1表选择(countltable_select[gr][ch])<br>
      　　scalefac_l[gr][ch][sfb],scalefac_s[gr][ch][sfb][window]), is_pos[sfb]<br>
      　　主数据位流（main_data）<br>
      　　哈夫曼编码数据(huffmancodebits()
<br>
　　(7)　辅助数据<br>
      　　辅助位(ancillary_bit): 由用户定义。<br>
      　　辅助位的数目(no_of_ancillary_bits): 等于一个音频帧中可用的位数减去实际用于头错误检查和音频数据的位数。<br>
        　　在层Ⅰ和层Ⅱ中, no_of_ancillary_bits对应于音频数据的尾端到下一个头开始处间的距离。在层Ⅲ中，no_of_ancillary_bits对应于huffman_code_bits的尾端与位流中下一帧的main_data_begin指针指向的位置之间的距离。<br>
      <b><font color="#003399">　　3.3.6.3 音频编码过程<a name="03"></a> 　　 </font></b><br>
      　　ISO/IEC 11172-3(MPEG_音 频)的编码器框图如图3.17。其算法实际上是一个心理声学算法。下面简介其算法。<br>
      <font color="#003399">　　(1)滤波器组</font><br>
      　　滤波器组实现时域到频域的转换。MPEG-音频算法中使用了两个滤波器组:一个多相滤波器组和一个混合多相/MDCT滤波器组。(MDCT:改进的离散余弦变换)。滤波器的输出是量化的值。<br>
      　　在层Ⅰ和层Ⅱ，使用了一个有32个子带的滤波器组。在每个子带中，把12个或36个抽样作为一组进行处理。而在层Ⅲ中，滤波器组的分辨率与信号有关，或是6×32,或是18×32的频带。在6×32的频率抽样的情况下，每3组频率被分别量化。<br>
      　　另外，建议在编码器的输入端加入一个高通滤波器，其截止频率范围应在2～10HZ, 以避免对最低子带的高数据位的需求。
      <br>
      <font color="#003399">　　(2)数据或噪声分配</font><br>
      　　此分配器既要考虑滤波器组的输出样本。又要考虑由心理声学模型（见下面的详细论述）输出的信号掩蔽比，并调整位分配（层Ⅰ和层Ⅱ）或噪声分配（层Ⅲ），以同时满足位率需求和掩蔽需求。<br>
      　　在层Ⅰ和层Ⅱ，在每个子带中为每个抽样（或一组抽样）分配一些数据位；在层Ⅲ中，要控制的变量是引入的噪声。两种情况下，其结果都是一系列的量化参数和量化了的输出抽样，它们再被输入到位流格式化器。
<br>
      <font color="#003399">　　(3)位流格式化器</font><br>
      　　位流格式化器取得量化的滤波器输出，与位分配（层Ⅰ和层Ⅱ）或噪声分配（层Ⅲ），以及其它所需的辅助信息一起，用有效的方式进行编码和格式化。在层Ⅲ中，哈夫曼编码也在这里进行，这些哈夫曼码是可变长度码，允许以更多的有效位流来表示量化的抽样，当然就增加了复杂度。
<br>
      <font color="#003399">　　(4)心理声学模型</font><br>
      　　MPEG_音频有两个心理声学模型, 为滤波器组中的每个频带计算刚能觉察的噪声电平。在实践中，一般层Ⅰ和层Ⅱ用一个模型，层Ⅲ用另外一个。两模型的输出均是每个频带(层Ⅰ和Ⅱ)或一组频带(层Ⅲ)的信号掩蔽比(SMR)。<br>
      　　<font color="#003399">(5)音频的编码</font><br>
      　　*层Ⅰ的编码<br>
      <font color="#003399">　图3.20 层Ⅰ, Ⅱ编码器流程图</font> <br>
      　<img src="../../../../images/chatp/chap03/098.gif"> <br>
      <font color="#003399">　层Ⅰ格式的概貌如图3.21。</font>
<table width="100%" border="0" cellspacing="1" cellpadding="1">
        <tr>
          <td bgcolor="#006699">
            <table width="100%" border="0" cellspacing="1" cellpadding="1">
              <tr bgcolor="#FFFFFF"> 
                <td class="text"> 
                  <div align="center">头</div>
                </td>
                <td class="text"> 
                  <div align="center">比特分配</div>
                </td>
                <td class="text"> 
                  <div align="center">缩放因子</div>
                </td>
                <td class="text"> 
                  <div align="center">样本</div>
                </td>
                <td class="text"> 
                  <div align="center">辅助数据</div>
                </td>
              </tr>
            </table>
          </td>
        </tr>
      </table>
      　　*　层Ⅱ的编码<br>
      　　关于流程图, 请参看图3.20。 <br>
      　　层Ⅱ的格式如图3.22。<br>
      <font color="#003399">　图3.22　层Ⅱ的数据格式</font> <br>
      <table width="100%" border="0" cellspacing="1" cellpadding="1">
        <tr> 
          <td bgcolor="#006699"> 
            <table width="100%" border="0" cellspacing="1" cellpadding="1">
              <tr bgcolor="#FFFFFF"> 
                <td class="text"> 
                  <div align="center">头</div>
                </td>
                <td class="text"> 
                  <div align="center">比特分配</div>
                </td>
                <td class="text"> 
                  <div align="center">缩放因子选择</div>
                </td>
                <td class="text"> 
                  <div align="center">样本</div>
                </td>
                <td class="text"> 
                  <div align="center">辅助数据</div>
                </td>
              </tr>
            </table>
          </td>
        </tr>
      </table>
      　　与层Ⅰ的格式有以下几点不同:<br>
      　　<font color="#003399">(1)槽的长度是8。</font><br>
      <font color="#003399">　　(2)比特分配信息，缩放因子和相应的编码有关。</font> <br>
      　　* 层Ⅲ的编码<br>
        　　层Ⅲ的编码模型基于心理声学编码算法，着重以心理声学模型2为基础，并对心理声学模型2进行修改以适应层Ⅲ。<br>
        　　音频编码利用心理声学模型的结果, 为每一个样点分配适当的比特。<br>
        　　三层都有一些共同的步骤：分析子带滤波；因子等边信息编码；运用心理声学模型进行数据分配的计算；量化和编码；最后对数据进行格式化。<br>
        　　其中数据分配的计算是三层较大的差异。层Ⅰ和Ⅱ采用心理声学模型１，代入迭代循环；层Ⅲ修改心理声学模型２，计算平均有效比特数。层Ⅰ和层Ⅱ在大致类似的情况下，又因为子带中的样点数不同而略有差异。 
        <br>
        　　三层最大的不同在于量化和编码的方法。层Ⅰ比较简单，只需简单地查表并作乘加运算即可。层Ⅱ由于引入了成组信息，以及样点数的增多，不得不分别考虑。至于层Ⅲ，由于引入了可变位率，并且由于不同的块和不同的商类型等因素影响，采用了两层迭代循环，即外循环(失真循环)和内循环。<br>
      <b><font color="#003399">　　3.3.6.4. 心理声学模型</font></b><a name="04"></a><br>
      　　心理声学模型的计算要适用于相应的层次。而且算法有一定的灵活性。模型总共包括两大类：一是心理声学模型1，一是心理声学模型2。其中心理声学模型1对层Ⅰ和层Ⅱ都是有效的,并且在应用于这两层时,没有本质的差别.而且这个模型还可以应用于层Ⅲ;心理声学模型2是一个独立的模型,可以经过适当的调整来适应MPEG音频的任何层次.<br>
      　　心理声学模型主要用于编码，人们利用模型来判断哪些频率(或子带)中的音在整个音中对人们的影响最大。据此，在编码时，对这些音适当的增大量化级数，获得更细致的描述。<br>
      　　两个心理声学模型均通过计算信号掩蔽比来为编码器服务。心理声学模型Ⅰ通过频率的分析，得到音调和非音调成分，并求得掩蔽阈值，最后得到子带的信号掩蔽比。心理声学模型Ⅱ从能量入手，运用卷积等工具，得到了信号掩蔽比。 
      <br>
      　　<font color="#003399">下面对心理声学模型加以介绍：</font><br>
      　　时间匹配的音频数据：对于每帧有心理声学的预测。对于输送到心理声学模型的音频数据必须与编码的音频数据同步。心理声学模型必须计算出通过滤波器的声音数据的滞后时间以及编移量，分析窗口对这些数据进行分析。<br>
      　　将音频转化成频率域表示法：心理声学模型应当采用独立的时间-频率映射代替多相结构的滤波器。因为它要求更高精度的频率分辨率以便精确地计算掩蔽的域值。两个模型均采用傅立叶变换来表示这种映射。<br>
      　　心理声学模型I在第一层次中采用了512个取样分析窗口，在第二层次和第三层次中各采用了1024个取样窗口。这是因为在层次I和II中分别只有384个取样。虽然理想状态是分析窗口数与编码取样数相适应，但按以上处理不会对心理声学预测产生太大的影响。<br>
      　　心理声学模型II在三个层次中都采用了1024个取样窗口。对于层次I，这个模型在心理声学窗口集中了一帧的384个音频样本。对于层次II和III，这个模型在每帧进行了两个1024点心理声学计算。这个模型利用较高的信号掩蔽比（SMR）将每个子带计算结果联系在一起。这就为每个子带选择了较低的噪音掩蔽域值。<br>
      　　将音调类与非音调类成分分开：因为对于音调成分和非音调成分的屏蔽能力不同，所以这两种模型均可将其分开。<br>
      　　心理声学模型I根据音频能谱的局部峰值对音调部分进行识别。当对音调部分进行处理后，模型I把每个临界频带的剩余能谱值归结为单独的非音调部分。每个集中的非音调成分的频率指数与临界频带频率的几何平均值最接近。<br>
      　　心理声学模型I对音调和非音调部分不进行严格划分，而是将音调指标表达成频率的一个函数。这个指标就是区别是音调还是噪音的标准。只有音调指标可预测才可能运用 
      这个模型。模型II采用前面提到的两个分析窗口的数据来预测。由于这一模型是基于许多数据之上，在区分音调成分和非音调成分上也许更加合理。<br>
      　　这两个模型都要依经验预置一个绝对的掩蔽域值。这个域值在可闻声的下限。模型I在每个子带中选择了一个最小的掩蔽域值，当子带频率较低时子带与临界频带严格对应，这种处理很有效；但当子带的频率较高时，由于这种频率的临界频带包含了N个子带，这样的处理就欠妥了。<br>
      　　模型II选择域值的方法是：当频带在频率范围内与临界频带无多大关系时，取最小掩蔽域值；当频带与临界频带严格对应时，选择平均掩蔽域值。模型II对于高频子带有与低频子带同样的精度。<br>
      　　层次I、II &amp; III 算法。<br>
      　　三层算法的编码效率和复杂成度依次增加，分别适用于不同的场合。第层算法包含最基本的一些模块：子带分析、心理声学模型、动态比特分配、分组压扩量化、复接成帧。第二层算法对比特分配信息，归一化因子和子带样值提供了进一步的压码，且帧结构也不再相同了。第三层算法采用了混合滤波器以提高分辨率，还用到了非均匀量化、自适应分块以及熵编码等技术。<br>
      　　层次I、II算法中，子带分析采用了WOA标准，心理声学模型采用了掩蔽域值预测模型（模型I），量化器采用了自适应分组压扩技术。层次I、II算法的帧格式中，头信息包含了音频源信号的采样频率、编码器的层号、目标比特率以及是否使用了联合立体声编码等信息。层次I算法一帧由384个样值经编码得到的，一帧所含的比特数由目标比特率、音频信号采样率及层数共同确定。层次II算法中，一帧含1152个音频样值。归一化因子信息分成两部分：第一部分是归一化因子选择信息，各子带包含的三个归一化因子并不都传，传几个以及传哪个都由归一化因子选择信息表确定；第二部分才是用于传输的那些归一化因子。层次II算法对比特分配信息也进行压缩，每个子带所占比特数是低频子带占得多，高频子带占得少。同样的原则还适用于对子带样值的进一步压缩，32个子带中最高N个子带中的样值被完全丢弃，节省下来的比特可使人耳较敏感的低频区内的样值更精细。由于基本框架与层次一相同，故复杂度增加不多，但编码效果却有不少改善。<br>
      　　层次III的算法是从ASPEC（Auto Spectral Perceptual Eutrop Coping )和OCF ( Optimal 
      Coding in the Frequency Domal )算法演变出来的更为精确的算法。虽然它还是基于层次I和II同样的滤波器，层次III用修正的离散余弦变换（MDCT）对一些滤波器的缺陷进行了修正。<br>
      　　层次III的编码过程不同于多相结构的滤波器没有量化。MDCT变化无损，MOCT对子带输出在频率上再划分提高了频谱分辨滤。而且经过这样处理后，层次III编码能部分地排除由于多相结构滤波器而造成的阶梯效应。<br>
      　　层次III有两个不同的MDCT块长度：一个18个样本的长块和一个6样本的短块。在相邻的转换窗口有50%的重叠，因此窗口尺寸分别为36和12。长块对有静止字符的音频信号有较大的频率分辨率，但短块的长度对瞬态过渡提供了更好的时间分辨率。对于给字帧的音频样本，MDCT能有相同的块长度（长块或短块）或混合的块模式。在混合模式下，低频子带的MDCT有长块；对于30个上部的子带，MDCT有短块。<br>
      　　这个模式为低频提供了更好的频率分辨率，而不是靠牺牲高频的时间分辨率来实现。<br>
      　　在长块和短块之间的转换不是瞬间完成的，专为块间转换设计了一个数据窗口。<br>
      　　因为为子带信号的MDCT过程提供了更好的频率分辨率，从而降低了时间分辨率。MDCT只对12或36个多相结构的滤波样本进行变换，因而处理音频取样的窗口有效时间为12或36倍。MDCT值的量化因而会产生偏差而造成声音的走调。<br>
      　　阶梯效应的衰减：层次III排除了一些因多相结构的滤波器的重叠频带而造成的人为因素。<br>
      　　非均匀量化，层次III量化器在量化之前使输入信号变为3/4次方，这样量化值在一定范围内信噪比更加一致。<br>
      　　比例因子频带：不象层次I和层次II，每个子带可有不同的比例因子，层次III采用比例因子频带，这些频带包括几个MDCT常数和近似的临界带宽。<br>
      　　熵编码：层次III使用变长度的Hulfman码对量化的取样进行编码，以取得更好的压缩效果。<br>
      　　使用&quot;比特库&quot;：编码器能够把多余的位送到&quot;比特库&quot;中，不够时又可以从&quot;比特库&quot;中取出。<br>
      <b><font color="#003399">　　3.3.6.5 音频的解码</font></b><a name="05"></a><br>
      　　在解码时, 首先要做的事是使解码器与位流同步。这可以通过搜索12位同步字来完成。在某些应用中，如果解码器已知标识符，层次和保护状态，就可以把头中的开始16位视作一个16位的同步字，因而可得到更可靠的同步。此外，为了便利同步操作，可能设置连续同步字。两个连续同步字的起始位置之间的距离等于N或N+1个槽。N值由层次决定:<br>
      　　层Ⅰ: N=12* 位率/采样频率<br>
      　　层Ⅱ、层Ⅲ：N=144*位率/采样频率<br>
      　　如果计算结果不是整数，就需要截取和填充。这可以由头中的填充位来判断。<br>
      　　如果位率索引等于&quot;0000&quot;, 则不指明确切的位率。<br>
      　　在确定了填充位后，再读出位流中的模式位，如果其值是&quot;01&quot;，则还要读出模式扩展位。<br>
      　　最后还要提一下CRC_校验。其发生多项式是：G(x)=X16+X15+X2+1。下表给出了哪些位将被进行CRC_校验:<br>
      　　层Ⅰ: 头中的16～31位, 位分配;<br>
      　　层Ⅱ: 头中的16～31位, 位分配, 缩放因子选择;<br>
      　　层Ⅲ: 头中的16～31位, 单声道时音频数据的0～135位;<br>
      　　其它模式音频数据的0～255位;<br>
      　　移位寄存器的初始状态是&quot;1111 1111 1111 1111&quot;。然后根据层数，将相应的要校验的位输入电路。每位都输入后，移动一位，直至最后一个移位操作完成。然后将输出b15…b0组成一个字与位流中的CRC_校验字比较。不同则表明有错误发生。<br>
      　　以上是解码部分中对所有层都适用的分共部分，下面分层讲述。
      <br>
      <font color="#003399">　　(1) 层Ⅰ的解码过程</font><br>
      　　在做完公共部分后, 要读取位分配信息，以及第一个子带的缩放因子。<br>
      　　下面是解码流程图：<br>
      <font color="#003399">　图3.23　层Ⅰ和层Ⅱ的解码流程图</font> <br>
      　　<img src="../../../../images/chatp/chap03/099.gif" width="199" height="336"><br>
      　　主要包括子带样点的逆量化和合成子带滤波器两大部分。<br>
      <font color="#003399">　　(2) 层Ⅱ的解码</font><br>
      　　由于层Ⅱ的编码比层Ⅰ更复杂, 因此解码也就相应复杂一些。<br>
      　　主要包括位分配解码、缩放因子选择信息解码、子带样点的逆量化、合成子带滤波器等四部分。<br>
      <font color="#003399">　　(3) 层Ⅲ的解码过程</font><br>
      　　层Ⅲ的解码过程是最复杂的,如图3.24。<br>
      <font color="#003399">　图3.24 层Ⅲ解码器流程图</font>　 <br>
      　　<img src="../../../../images/chatp/chap03/100.gif" width="241" height="575"> 
      <br>
      　　主要包找同步、附加信息、主数据开始、缩放因子、哈夫曼解码、逆量化器、逆量化和全缩放公式、重排序、立体声处理（包括MS_立体声模式和强度立体声模式）、合成滤波器组、混迭信号的减除等。<br>
      　　有关各层解码的详细过程请读者参考《MPEG 标准中图像伴音的编码/解码算法》一书。<br>
      　　<font color="#003399">音频编码的国际标准：<br>
      　　当前编码技术发展的一个重要方向就是综合现有的编码技术，制定全球的统一标准，使信息管理系统具有普遍的互操作性并确保了未来的兼容性。国际上，对语音信号压缩编码的审议在CCITT下设的第十五研究组进行，相应的建议为G系列，多由ITU发表。 
      <br>
      　　国际电报电话咨询委员会（CCITT）和国际标准化组织（ISO）先后提出一系列有关音频编码的建议。<br>
      　　1972年首先制定了G.711 64Kbps （A）律PCM编码标准。<br>
      　　1984年又公布了G.721标准（1986年修订）。它采用的是自适应差分脉冲编码（ADPCM），数据率为32Kbps。这两个标准适用于200～3400Hz窄带话音信号，已用于公共电话网。针对宽带语音（50～7KHz）。<br>
      　　CCITT制定的G.722编码标准，它的数据率为64Kbps。它可用于综合业务数据网（ISDN）的B通道上传输音频数据。之后公布的G.723建议中码率为40kbps和24kbps, 
      G.726中码率为16kbps。<br>
      　　CCITT于1990年通过了16-40kbps镶嵌式ADPCM标准G.727。低码率、短延时、高质量是人们期望的目标。在AT&amp; T 
      Bell实验室，16Kbps短延时码激励（LD-CELP）编码方案的基础上，经优化。<br>
      　　CCITT在1992年和1993年分别公布了浮点和定点算法的G.728标准。该算法延时小于2ms，话音质量可达MOS4分以上。ISO的运动图像专家组在制定运动图像编码标准的同时，为图像伴音制定了20KHZ带宽的128Kbps标准。<br>
      　　1988年欧州数字移动通信GSM制定了泛美数字移动通信网的13Kbps长时预测规则码激励（RPE-LTP）语音编码标准。1989年北美蜂窝电话工业组织（CTIA）公布了北美数字移动通信标准。它采用自适应码本激励。日本的数字移动通信标准是6.7Kbps的VSELP（矢量和激励线性预测）。</font><br>
      <br>
    </td>
  </tr>
</table>
</body>
</html>
