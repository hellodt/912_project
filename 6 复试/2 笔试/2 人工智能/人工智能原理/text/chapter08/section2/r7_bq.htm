<html>
<head>
<title>人工智能原理</title>
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<link rel="stylesheet" href="../../../css/text.css" type="text/css">
</head>
<body bgcolor="#FFFFFF" class="bg">
<table width="100%" border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td class=text> 　　<b>常识必然认识性</b>：<br>
      　　我们需要的下一个特性是：承认给定的系统事实。我们通过给逻辑加上其它的推理规则来表达这个属性。这个规则被称为：认识必要性（epistemic 
      necessitation）。设φ是一个有效的公式，此规则使得当前的智能体可以去推断Κα(φ)。我们可以把这个规则的推理表示为：<br>
      　　　　from ├φ infer Κα(φ) （6）<br>
      　　这里&quot;├ φ&quot;表示：φ是永真式，&quot;infer&quot;表示：推导出 <br>
      　　<b>规则无所不知性</b><br>
      　　因为假言推理是命题逻辑中唯一需要推理的规则，分布公理和否定内省规则使我们有如下的结论：一个智能体知道所有其知识的命题结果。即，任何一个智能体知道上述的这些公理和数理逻辑基本规则。也就是说，一个智能体承认某一个公理系统，承认一些基本的数理逻辑规则。从逻辑上讲，它是无所不能的。我们可以用一个推理规则来表达这个事实： 
      <br>
      　　　　from φ├ψ and from Κα(φ) infer Κα(ψ) （7）<br>
      　　这个规则的等价的形式为：<br>
      　　　　from ├(φ<img src="../../../img/chap08/symbol02.gif" width="12" height="16">ψ) 
      infer Κα(φ)<img src="../../../img/chap08/symbol02.gif" width="12" height="16">Κα(ψ) 
      （8）<br>
      　　逻辑上全知对于有限的智能体来说似乎有些不切实际，因为它们毕竟不能从它们可能显式知道的事情中推理出所有的结果。如果一个智能体不能推导出一个命题（尽管是跟随它所知道的其它命题），能说它知道那个命题吗？这依赖于我们怎么看待'知道'。例如，我们可能有一个柏拉图式的知识观点，在这个观点中，按照定义，一个智能体知道其知识的所有结果，尽管它可能并不显式的理解它们。虽然逻辑全知看起来条件太强，它在近似的意义下还是很有用处的，因为智能的智能体还是要进行一些推理工作的。<br>
      　　从逻辑全知公式，我们可以推出：<br>
      　　　　Κ(α,(φ∧ψ))<img src="../../../img/chap08/symbol03.gif" width="18" height="16">Κ(α,φ)∧Κ(α,ψ) 
      （9）<br>
      　　<font color="#0000FF">注意</font>：从上式可以看出，算子Κ满足合取运算的分配规则。但是它不满足析取运算的分配规则，因为如下的规则不成立：Κ(α,(φ∨ψ))<img src="../../../img/chap08/symbol02.gif" width="12" height="16">Κ　　　　　(α,φ)∨Κ(α,ψ)。<br>
      　　当前的智能体可以用表达式：Κ(α,φ)∨Κ(α,～φ)，表示其它的智能体知道或者不知道φ，而不管自己是否知道φ。 </td>
  </tr>
</table>

</body>
</html>
